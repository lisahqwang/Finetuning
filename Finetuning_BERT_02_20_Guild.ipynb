{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6703077a",
      "metadata": {
        "id": "6703077a"
      },
      "source": [
        "## Semantic Role Labelling with BERT (~3 hours)\n",
        "\n",
        "The goal of this notebook is to train and evaluate a semantic role labeling (SRL) system. For each input token (word), the system will predict a B-I-O tag and a predicate, as illustrated in the following example:\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "\n",
        "|The|judge|scheduled|to|preside|over|his|trial|was|removed|from|the|case|today|.|             \n",
        "|---|-----|---------|--|-------|----|---|-----|---|-------|----|---|----|-----|-|             \n",
        "|B-ARG1|I-ARG1|B-V|B-ARG2|I-ARG2|I-ARG2|I-ARG2|I-ARG2|O|O|O|O|O|O|O|\n",
        "\n",
        "BIO = Beginning, Inside, and Outside", 
        "\n",
        "Arg1 = causer, Arg2 = effect, amount of effect", 
        "<br/>\n",
        "\n",
        "\n",
        "\n",
        "The SRL system will be implemented in [PyTorch](https://pytorch.org/). We will fine-tune the pretrained BERT model on the SRL task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For first-time BERT users, try this Google notebook to understand the basics: https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R_KQWOglzo-w"
      },
      "id": "R_KQWOglzo-w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BERT?\n",
        "\n",
        "- BERT is pre-trained using 64 TPUs over 4 days\n",
        "- BERT is a MLM (Masked Language Model) — masking a word in a sentence and forcing bidirectional token prediction\n",
        "- Transformer-based architecture\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "OuF8VhVanTbR"
      },
      "id": "OuF8VhVanTbR"
    },
    {
      "cell_type": "markdown",
      "id": "ff12fe94",
      "metadata": {
        "id": "ff12fe94"
      },
      "source": [
        "## Steps to Finetune: (<1 min)\n",
        "\n",
        "\n",
        "1. Encode the sentence with BERT\n",
        "2. Generate (contextualized) embedding\n",
        "3. Feed embeddings into classifier to predict tag\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that BERT is trained on two input sentences, seperated by [SEP], and on a next-sentence-prediction objective (in addition to the masked LM objective). To help BERT comprehend which sentence a given token belongs to, the original BERT uses a segment embedding, using A for the first sentene, and B for the second sentence 2.\n",
        "Because we are labeling only a single sentence at a time, we can use the segment embeddings to indicate the predicate position instead: The predicate is labeled as segment B (1) and all other tokens will be labeled as segment A (0).\n",
        "\n",
        "<img src=\"https://github.com/daniel-bauer/4705-f23-hw5/blob/main/bert_srl_model.png?raw=true\" width=400px>"
      ],
      "metadata": {
        "id": "8CISebgypeOQ"
      },
      "id": "8CISebgypeOQ"
    },
    {
      "cell_type": "markdown",
      "id": "68325e53",
      "metadata": {
        "id": "68325e53"
      },
      "source": [
        "## Setup: GCP, Jupyter, PyTorch, GPU (<1 min)\n",
        "\n",
        "To make sure that PyTorch is available and can use the GPU, run the following cell which should return True. If it doesn't, make sure the GPU drivers and CUDA are installed correctly.\n",
        "\n",
        "GPU support is required for this notebook -- you will not be able to finetune BERT on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "aad94eef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aad94eef",
        "outputId": "e014272a-4f54-46b8-df39-48713587b694"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe1c8ac",
      "metadata": {
        "id": "3fe1c8ac"
      },
      "source": [
        "## Dataset: Ontonotes 5.0 English SRL annotations (5 mins)\n",
        "\n",
        "Download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0c2f2645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c2f2645",
        "outputId": "4ded439f-3132-4ea1-ba22-9f6ecb3f12d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-11 07:04:16--  https://storage.googleapis.com/4705-bert-srl-data/ontonotes_srl.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 74.125.126.207, 142.251.183.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12369688 (12M) [application/zip]\n",
            "Saving to: ‘ontonotes_srl.zip.1’\n",
            "\n",
            "ontonotes_srl.zip.1 100%[===================>]  11.80M  44.4MB/s    in 0.3s    \n",
            "\n",
            "2025-02-11 07:04:16 (44.4 MB/s) - ‘ontonotes_srl.zip.1’ saved [12369688/12369688]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://storage.googleapis.com/4705-bert-srl-data/ontonotes_srl.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14aad58e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14aad58e",
        "outputId": "ae0f2005-23e4-4fe9-bf55-e2472b1a35ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ontonotes_srl.zip\n",
            "  inflating: propbank_dev.tsv        \n",
            "  inflating: propbank_test.tsv       \n",
            "  inflating: propbank_train.tsv      \n",
            "  inflating: role_list.txt           \n"
          ]
        }
      ],
      "source": [
        "! unzip ontonotes_srl.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now downloaded the four files:\n",
        "1. propbank_dev.tsv\n",
        "2. propbank_test.tsv\n",
        "3. propbank_train.tsv\n",
        "4. role_list.txt"
      ],
      "metadata": {
        "id": "PWLuaUitwJwK"
      },
      "id": "PWLuaUitwJwK"
    },
    {
      "cell_type": "markdown",
      "id": "8cae08a9",
      "metadata": {
        "id": "8cae08a9"
      },
      "source": [
        "The data has been pre-processed in the following format. There are three files:\n",
        "\n",
        "`propbank_dev.tsv`\t`propbank_test.tsv`\t`propbank_train.tsv`\n",
        "\n",
        "Each of these files is in a tab-separated value format. A single predicate/argument structure annotation consists of four rows. For example\n",
        "\n",
        "```\n",
        "ontonotes/bc/cnn/00/cnn_0000.152.1\n",
        "The     judge   scheduled       to      preside over    his     trial   was     removed from    the     case    today   /.\n",
        "                schedule.01\n",
        "B-ARG1  I-ARG1  B-V     B-ARG2  I-ARG2  I-ARG2  I-ARG2  I-ARG2  O       O       O       O       O       O       O\n",
        "```\n",
        "\n",
        "* The first row is a unique identifier (1st annotation of the 152nd sentence in the file ontonotes/bc/cnn/00/cnn_0000).\n",
        "* The second row contains the tokens of the sentence (tab-separated).\n",
        "* The third row contains the probank frame name for the predicate (empty field for all other tokens).\n",
        "* The fourth row contains the B-I-O tag for each token.\n",
        "\n",
        "The file `rolelist.txt` contains a list of propbank BIO labels in the dataset (i.e. possible output tokens). This list has been filtered to contain only roles that appeared more than 1000 times in the training data.\n",
        "We will load this list and create mappings from numeric ids to BIO tags and back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "dc4c027a",
      "metadata": {
        "id": "dc4c027a"
      },
      "outputs": [],
      "source": [
        "role_to_id = {}\n",
        "with open(\"role_list.txt\",'r') as f:\n",
        "    role_list = [x.strip() for x in f.readlines()]\n",
        "    role_to_id = dict((role, index) for (index, role) in enumerate(role_list))\n",
        "    role_to_id['[PAD]'] = -100\n",
        "\n",
        "    id_to_role = dict((index, role) for (role, index) in role_to_id.items())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07cdf31",
      "metadata": {
        "id": "f07cdf31"
      },
      "source": [
        "Note that we are also mapping the '[PAD]' token to the value -100. This allows the loss function to ignore these tokens during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae727bf",
      "metadata": {
        "id": "aae727bf"
      },
      "source": [
        "## Part 1 - Data Preparation (20 mins)\n",
        "\n",
        "Before you can build the SRL model, you first need to preprocess the data.\n",
        "\n",
        "\n",
        "### 1.1 - Tokenization (1 min)\n",
        "\n",
        "One challenge is that the pre-trained BERT model uses subword (\"WordPiece\") tokenization, but the Ontonotes data does not. Fortunately Huggingface transformers provides a tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8a2d7d03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2d7d03",
        "outputId": "9c39fa9b-2775-4c67-9bdb-5ab486d80781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'is',\n",
              " 'an',\n",
              " 'un',\n",
              " '##bel',\n",
              " '##ie',\n",
              " '##va',\n",
              " '##bly',\n",
              " 'boring',\n",
              " 'test',\n",
              " 'sentence',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenizer.tokenize(\"This is an unbelievably boring test sentence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac1924bd",
      "metadata": {
        "id": "ac1924bd"
      },
      "source": [
        "To demonstrate what the post-trained BERT model will need as inputs, I will show an example.\n",
        "\n",
        "The following function takes a list of tokens and a list of B-I-O labels of the same length as parameters, and returns a new token / label pair.\n",
        "\n",
        "\n",
        "```\n",
        ">>> tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)\n",
        "(['the',\n",
        "  'fancy',\n",
        "  '##ful',\n",
        "  'penguin',\n",
        "  'dev',\n",
        "  '##oured',\n",
        "  'yu',\n",
        "  '##mmy',\n",
        "  'fish',\n",
        "  '.'],\n",
        " ['B-ARG0',\n",
        "  'I-ARG0',\n",
        "  'I-ARG0',\n",
        "  'I-ARG0',\n",
        "  'B-V',\n",
        "  'I-V',\n",
        "  'B-ARG1',\n",
        "  'I-ARG1',\n",
        "  'I-ARG1',\n",
        "  'O'])\n",
        "\n",
        "```\n",
        "1. Iterate through each word in sentence\n",
        "2. Invoke tokenizer\n",
        "3. return tokenized sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8140fc39",
      "metadata": {
        "id": "8140fc39"
      },
      "outputs": [],
      "source": [
        "def tokenize_with_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces.\n",
        "\n",
        "    Args:\n",
        "        sentence: List of words\n",
        "        text_labels: List of B-I-O labels for each word\n",
        "        tokenizer: BERT tokenizer\n",
        "\n",
        "    Returns:\n",
        "        tokenized_sentence: List of subword tokens\n",
        "        labels: List of B-I-O labels matching the subword tokens\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    # todo\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "      # use tokenizer to tokenize\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      tokenized_sentence.extend(word_tokens)  # concatenate a list\n",
        "      # handle subword\n",
        "      if len(word_tokens) > 0:\n",
        "        # First subword gets the original label\n",
        "        labels.append(label)\n",
        "\n",
        "        # If label starts with B-, remaining subwords get I- prefix\n",
        "        # nested len(word_tokens) to avoid generating multiple B- tokens\n",
        "        if len(word_tokens) > 1:\n",
        "          if label.startswith('B-'):\n",
        "              base_label = 'I-' + label[2:]  # Convert B- to I- for same role\n",
        "          else:\n",
        "              base_label = label  # Keep I- or O as is\n",
        "          # Add the label for each additional subword\n",
        "          labels.extend([base_label] * (len(word_tokens) - 1))\n",
        "\n",
        "\n",
        "    return tokenized_sentence, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f748d120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f748d120",
        "outputId": "3115d929-b96f-42e5-8e71-13886854ecd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['the',\n",
              "  'fancy',\n",
              "  '##ful',\n",
              "  'penguin',\n",
              "  'dev',\n",
              "  '##oured',\n",
              "  'yu',\n",
              "  '##mmy',\n",
              "  'fish',\n",
              "  '.'],\n",
              " ['B-ARG0',\n",
              "  'I-ARG0',\n",
              "  'I-ARG0',\n",
              "  'I-ARG0',\n",
              "  'B-V',\n",
              "  'I-V',\n",
              "  'B-ARG1',\n",
              "  'I-ARG1',\n",
              "  'I-ARG1',\n",
              "  'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77bb9076",
      "metadata": {
        "id": "77bb9076"
      },
      "source": [
        "### 1.2 Loading the Dataset (10 mins)\n",
        "\n",
        "Next, we are creating a PyTorch [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. This class acts as a contained for the training, development, and testing data in memory. For each annotation you start with  the tokens in the sentence, and the BIO tags.\n",
        "\n",
        "1. call the `tokenize_with_labels` function to tokenize the sentence.\n",
        "2. Add the (token, label) pair to the self.items list.\n",
        "\n",
        "\\_\\_len\\_\\_(self) method that returns the total number of items.\n",
        "\n",
        "\\_\\_getitem\\_\\_(self, k) method that returns a single item in a format BERT will understand.\n",
        "\n",
        "We will return a dictionary in the following format:\n",
        "\n",
        "```\n",
        "{'ids': token_tensor,\n",
        " 'targets': tag_tensor,\n",
        " 'mask': attention_mask_tensor,\n",
        " 'pred': predicate_indicator_tensor}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9f5bd32f",
      "metadata": {
        "id": "9f5bd32f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SrlData(Dataset):\n",
        "\n",
        "    def __init__(self, filename):\n",
        "\n",
        "        super(SrlData, self).__init__()\n",
        "\n",
        "        self.max_len = 128 # the max number of tokens inputted to the transformer.\n",
        "\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "        self.items = []\n",
        "        '''\n",
        "        todo:\n",
        "            read in data specified by filename\n",
        "            1. call the `tokenize_with_labels` function to tokenize the sentence.\n",
        "            2. Add the (token, label) pair to the self.items list.\n",
        "        '''\n",
        "        with open(filename, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # Process 4 lines at a time (format of the data file) Check Dataset data description for more info.\n",
        "            for i in range(0, len(lines), 4):\n",
        "                # Skip if we don't have complete 4 lines\n",
        "                if i + 3 >= len(lines):\n",
        "                    break\n",
        "\n",
        "                # Get each line\n",
        "                # file id, sentence token, predicate information, BIO tags\n",
        "                sentence = lines[i + 1].strip().split('\\t')  # tokens\n",
        "                predicates = lines[i + 2].strip().split('\\t')  # predicate info\n",
        "                labels = lines[i + 3].strip().split('\\t')  # BIO tags\n",
        "\n",
        "                # Tokenize\n",
        "                tokenized_sentence, tokenized_labels = tokenize_with_labels(\n",
        "                    sentence, labels, self.tokenizer\n",
        "                )\n",
        "\n",
        "                # Add to items list\n",
        "                # self.items.append({\n",
        "                #     'tokens': tokenized_sentence,\n",
        "                #     'labels': tokenized_labels,\n",
        "                #     'predicates': predicates\n",
        "                # })\n",
        "                self.items.append({\n",
        "                    'tokens': tokenized_sentence,\n",
        "                    'labels': tokenized_labels\n",
        "                })\n",
        "        # end of init\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the total number of items\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        # return a single item in a BERT format\n",
        "        '''\n",
        "        * We need to process the sentence by adding '[CLS]'\" as the first token and '[SEP]' as the last token. The need to pad the token sequence to 128 tokens using the \"[PAD]\" symbol.\n",
        "        This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence).\n",
        "\n",
        "        * We need to create an *attention mask*, which is a sequence of 128 tokens indicating the actual input symbols (as a 1) and \\[PAD\\] symbols (as a 0).\n",
        "        * We need to create a *predicate indicator* mask, which is a sequence of 128 tokens with at most one 1, in the position of the \"B-V\" tag.\n",
        "        All other entries should be 0. The model will use this information to understand where the predicate is located.\n",
        "\n",
        "        * Finally, we need to convert the token and tag sequence into numeric indices. For the tokens, this can be done using the `tokenizer.convert_tokens_to_ids` method.\n",
        "        For the tags, use the `role_to_id` dictionary.\n",
        "        Each sequence must be a pytorch tensor of shape (1,128). You can convert a list of integer values like this `torch.tensor(token_ids, dtype=torch.long)`.\n",
        "        '''\n",
        "        item = self.items[k]\n",
        "        tokens = item['tokens']\n",
        "        labels = item['labels']\n",
        "\n",
        "        # Truncate if longer than max_len - 2 (to account for [CLS] and [SEP])\n",
        "        max_seq_length = self.max_len - 2\n",
        "        tokens = tokens[:max_seq_length]\n",
        "        labels = labels[:max_seq_length]\n",
        "\n",
        "\n",
        "        # Add special tokens\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        labels = ['O'] + labels + ['O']  # Using 'O' for special tokens\n",
        "        attention_mask = [1] * len(tokens) # (1 for real tokens, 0 for padding)\n",
        "\n",
        "        # Pad sequences to exact length\n",
        "        padding_length = self.max_len - len(tokens)\n",
        "        if padding_length > 0:\n",
        "            tokens = tokens + ['[PAD]'] * padding_length\n",
        "            labels = labels + ['[PAD]'] * padding_length\n",
        "            attention_mask = attention_mask + [0] * padding_length\n",
        "\n",
        "        # Convert tokens to ids\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Create predicate indicator\n",
        "        predicate_mask = [1 if label == 'B-V' else 0 for label in labels]\n",
        "\n",
        "        # Convert labels to ids\n",
        "        label_ids = [role_to_id[label] if label in role_to_id else role_to_id['[PAD]'] for label in labels]\n",
        "\n",
        "\n",
        "        # Convert to tensors\n",
        "        return {\n",
        "            'ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(label_ids, dtype=torch.long),\n",
        "            'pred': torch.tensor(predicate_mask, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0b476d",
      "metadata": {
        "id": "da0b476d"
      },
      "outputs": [],
      "source": [
        "# Reading the training data usually takes 10 mins, entire data is preprocessed offline\n",
        "data = SrlData(\"propbank_train.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0730aff9",
      "metadata": {
        "id": "0730aff9"
      },
      "source": [
        "## 2. Model Definition (10 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb11f9c0",
      "metadata": {
        "id": "cb11f9c0"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module, Linear, CrossEntropyLoss\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da684eff",
      "metadata": {
        "id": "da684eff"
      },
      "source": [
        "Since we will use the BERT model, we define layers and forward function ourselves.\n",
        "\n",
        "1. Freeze BERT parameters\n",
        "2. Define linear classifier\n",
        "3. Classify the tags we created\n",
        "\n",
        "\n",
        "Module, Linear, and Loss calculation functions are all imported from the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37515695",
      "metadata": {
        "id": "37515695"
      },
      "outputs": [],
      "source": [
        "class SrlModel(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(SrlModel, self).__init__()\n",
        "\n",
        "        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # The following two lines would freeze the BERT parameters and allow us to train the classifier by itself.\n",
        "        # We are fine-tuning the model, so you can leave this commented out!\n",
        "        # for param in self.encoder.parameters():\n",
        "        #    param.requires_grad = False\n",
        "\n",
        "        # The linear classifier head, see model figure in the introduction.\n",
        "        self.classifier = Linear(768, len(role_to_id))\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, pred_indicator):\n",
        "\n",
        "        # This defines the flow of data through the model\n",
        "\n",
        "        # Note the use of the \"token type ids\" which represents the segment encoding explained in the introduction.\n",
        "        # In our segment encoding, 1 indicates the predicate, and 0 indicates everything else.\n",
        "        bert_output =  self.encoder(input_ids=input_ids, attention_mask=attn_mask, token_type_ids=pred_indicator)\n",
        "\n",
        "        enc_tokens = bert_output[0] # the result of encoding the input with BERT\n",
        "        logits = self.classifier(enc_tokens) #feed into the classification layer to produce scores for each tag.\n",
        "\n",
        "        # Note that we are only interested in the argmax for each token, so we do not have to normalize\n",
        "        # to a probability distribution using softmax. The CrossEntropyLoss loss function takes this into account.\n",
        "        # It essentially computes the softmax first and then computes the negative log-likelihood for the target classes.\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create new model and store weights in GPU memory (5 mins)"
      ],
      "metadata": {
        "id": "fhPWmj1QRuhg"
      },
      "id": "fhPWmj1QRuhg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba23ec3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "93bd7099ad4f4fd5a18b93d27247c0bc",
            "44a9ac7c64a84fca8f5283ad0e856406",
            "3eb43ac363c64a8f8591b095f6031124",
            "b929cca6f52d4fd885d51487dc879b07",
            "0642828a79604cd49167cc673cab6109",
            "14746cf2be3f4b7fbbf77b2659ec35bb",
            "e13c9ccf399f4624bdcbf833760e6d43",
            "c50ded0e55b84ea986c8fea8aed388f3",
            "9bdb93578f27452d9392ecd29c44a018",
            "74a54fd4d01549f58161edd13281756e",
            "849127cb883140758caeb4db0fbeff27"
          ]
        },
        "id": "ba23ec3f",
        "outputId": "a8eec739-41af-41de-befe-88b90fd65f73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93bd7099ad4f4fd5a18b93d27247c0bc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = SrlModel().to('cuda') # create new model and store weights in GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### View a subset of the post-processed items in the Dataset (optional)"
      ],
      "metadata": {
        "id": "U2CFpNsFUGtI"
      },
      "id": "U2CFpNsFUGtI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1805480",
      "metadata": {
        "id": "f1805480"
      },
      "outputs": [],
      "source": [
        "sample = data[0]  # Get first item\n",
        "# Move tensors to GPU\n",
        "ids = sample['ids'].unsqueeze(0).to('cuda')  # Add batch dimension\n",
        "mask = sample['mask'].unsqueeze(0).to('cuda')\n",
        "pred = sample['pred'].unsqueeze(0).to('cuda')\n",
        "targets = sample['targets'].unsqueeze(0).to('cuda')\n",
        "outputs = model(ids, mask, pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ids shape:\", ids.shape)\n",
        "print(\"mask shape:\", mask.shape)\n",
        "print(\"pred shape:\", pred.shape)\n",
        "\n",
        "for i in range(0, 1): # view a subset of n items in the dataset\n",
        "   print(data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vv1QTkckZGa",
        "outputId": "34d7ca04-9bd8-435e-e5f7-159122e2186f"
      },
      "id": "2vv1QTkckZGa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ids shape: torch.Size([1, 128])\n",
            "mask shape: torch.Size([1, 128])\n",
            "pred shape: torch.Size([1, 128])\n",
            "{'ids': tensor([  101,  2057, 26438,  2135, 13260,  2017,  2000,  3422,  1037,  2569,\n",
            "         3179,  1997,  2408,  2859,  1012,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0]), 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([   4,    5,   19,   43,   28,    6,    8,   32,   32,   32,   32,   32,\n",
            "          32,   32,    4,    4, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100]), 'pred': tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd7fd38b",
      "metadata": {
        "id": "cd7fd38b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Loss functions are very important in understanding how accurate our predictions are, or how good our models are.\n",
        "\n",
        "-\n",
        "\n",
        "Without training we would assume that all labels for each token (including the target label) are equally likely, so the negative log probability for the targets should be approximately: $$-\\ln(\\frac{1}{\\text{num_labels}})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5f881d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a5f881d",
        "outputId": "59105f3e-24e7-48aa-fe8c-7aa1bd6e3aeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.970291913552122"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import math\n",
        "-math.log(1 / len(role_to_id), math.e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a124d4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a124d4f",
        "outputId": "3fb6bffd-ddc1-40b9-fc35-022a63aef0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 3.8571228981018066\n"
          ]
        }
      ],
      "source": [
        "loss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n",
        "loss = loss_function(outputs.transpose(2,1), targets)\n",
        "print(f\"Initial loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the pre-training prediction score is 3.97 and the actual calculated loss function is not far from it: it is 4.06. We will revisit this score after we train the model."
      ],
      "metadata": {
        "id": "-Oepx3rkW9BG"
      },
      "id": "-Oepx3rkW9BG"
    },
    {
      "cell_type": "markdown",
      "id": "669a1037",
      "metadata": {
        "id": "669a1037"
      },
      "source": [
        "In the training step, we \"teach\" the model to do the following:\n",
        "\n",
        "Take a tensor\n",
        "```\n",
        "tensor([[ 1,  4,  4,  4,  4,  4,  5, 29, 29, 29,  4, 28,  6, 32, 32, 32, 32, 32,\n",
        "         32, 32, 30, 30, 32, 30, 32,  4, 32, 32, 30,  4, 49,  4, 49, 32, 30,  4,\n",
        "         32,  4, 32, 32,  4,  2,  4,  4, 32,  4, 32, 32, 32, 32, 30, 32, 32, 30,\n",
        "         32,  4,  4, 49,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6,  6, 32, 32,\n",
        "         30, 32, 32, 32, 32, 32, 30, 30, 30, 32, 30, 49, 49, 32, 32, 30,  4,  4,\n",
        "          4,  4, 29,  4,  4,  4,  4,  4,  4, 32,  4,  4,  4, 32,  4, 30,  4, 32,\n",
        "         30,  4, 32,  4,  4,  4,  4,  4, 32,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
        "          4,  4]], device='cuda:0')\n",
        "```\n",
        "\n",
        "Then use the id_to_role dictionary to decode to actual labels.\n",
        "\n",
        "```\n",
        "['[CLS]', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O', 'B-V', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'I-ARGM-TMP', 'O', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'O', '[SEP]', 'O', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'O', 'I-ARGM-TMP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'I-ARG2', 'O', 'I-ARG1', 'O', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c2dcc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69c2dcc0",
        "outputId": "9f44af26-11cc-42bc-d3b4-48fe511b1591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions tensor: tensor([[13, 16, 10, 51, 19, 34, 51, 36, 45, 36, 32, 16,  0, 41,  9,  9,  1,  1,\n",
            "          1,  1,  1,  1, 27, 27,  5,  1,  1,  1, 36,  1,  1, 36, 36, 36, 38, 45,\n",
            "         49, 49, 49, 36,  5,  5, 27, 40,  1,  1,  1,  1,  5,  5, 27, 10, 27,  1,\n",
            "          1,  1, 36, 36, 36, 36, 36, 36, 38, 45, 49, 16, 40,  5,  5, 36,  0,  1,\n",
            "          1, 25,  1,  1,  1, 27,  5, 34, 10,  1,  1,  1, 36,  1, 36,  1, 36, 36,\n",
            "         36, 38, 45, 49, 49, 40,  0,  5, 40,  1,  1,  1,  1,  1,  1,  1, 27, 45,\n",
            "         34, 10, 27,  1,  1,  1,  1,  1,  1,  1,  5,  5,  5, 45,  5, 40, 36, 27,\n",
            "         40, 27]], device='cuda:0')\n",
            "Predictions shape: torch.Size([1, 128])\n",
            "\n",
            "Sample predictions: ['B-ARGM-CAU', 'B-ARGM-EXT', 'B-ARG4', 'I-ARGM-LVB', 'B-ARGM-MNR', 'I-ARG4', 'I-ARGM-LVB', 'I-ARGM-ADV', 'I-ARGM-NEG', 'I-ARGM-ADV', 'I-ARG2', 'B-ARGM-EXT', 'I-ARGM-GOL', 'B-ARG3', 'B-ARG3', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', 'B-ARGM-LVB', 'B-ARGM-LVB', 'B-ARG0', '[CLS]', '[CLS]', '[CLS]', 'I-ARGM-ADV', '[CLS]', '[CLS]', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-DIR', 'I-ARGM-NEG', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-ADV', 'B-ARG0', 'B-ARG0', 'B-ARGM-LVB', 'I-ARGM-EXT', '[CLS]', '[CLS]', '[CLS]', '[CLS]', 'B-ARG0', 'B-ARG0', 'B-ARGM-LVB', 'B-ARG4', 'B-ARGM-LVB', '[CLS]', '[CLS]', '[CLS]', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-DIR', 'I-ARGM-NEG', 'I-ARGM-TMP', 'B-ARGM-EXT', 'I-ARGM-EXT', 'B-ARG0', 'B-ARG0', 'I-ARGM-ADV', '[CLS]', '[CLS]', 'B-ARGM-TMP', '[CLS]', '[CLS]', '[CLS]', 'B-ARGM-LVB', 'B-ARG0', 'I-ARG4', 'B-ARG4', '[CLS]', '[CLS]', '[CLS]', 'I-ARGM-ADV', '[CLS]', 'I-ARGM-ADV', '[CLS]', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-ADV', 'I-ARGM-DIR', 'I-ARGM-NEG', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-EXT', 'B-ARG0', 'I-ARGM-EXT', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', 'B-ARGM-LVB', 'I-ARGM-NEG', 'I-ARG4', 'B-ARG4', 'B-ARGM-LVB', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', '[CLS]', 'B-ARG0', 'B-ARG0', 'B-ARG0', 'I-ARGM-NEG', 'B-ARG0', 'I-ARGM-EXT', 'I-ARGM-ADV', 'B-ARGM-LVB', 'I-ARGM-EXT', 'B-ARGM-LVB']\n"
          ]
        }
      ],
      "source": [
        "# Get predictions\n",
        "predictions = torch.argmax(outputs, dim=2)\n",
        "print(\"\\nPredictions tensor:\", predictions)\n",
        "print(\"Predictions shape:\", predictions.shape)\n",
        "\n",
        "# Decode the predictions to labels\n",
        "pred_labels = [id_to_role[pred.item()] for pred in predictions[0] if pred.item() in id_to_role]\n",
        "print(\"\\nSample predictions:\", pred_labels)  # Show first 20 predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d52991",
      "metadata": {
        "id": "20d52991"
      },
      "source": [
        "## 3. Training loop (1 hour)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edba250d",
      "metadata": {
        "id": "edba250d"
      },
      "source": [
        "pytorch provides a DataLoader class that can be wrapped around a Dataset to easily use the dataset for training. The DataLoader allows us to easily adjust the batch size and shuffle the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ecd7448",
      "metadata": {
        "id": "2ecd7448"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(data, batch_size = 32, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7496c62",
      "metadata": {
        "id": "e7496c62"
      },
      "source": [
        "The following cell contains the main training loop.\n",
        "\n",
        "1. Computes the accuracy, loss for each batch\n",
        "2. Reports the average accuracy, average loss after the epoch\n",
        "\n",
        "The accuracy is the number of correctly predicted token labels out of the number of total predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7abad9",
      "metadata": {
        "scrolled": true,
        "id": "7a7abad9"
      },
      "outputs": [],
      "source": [
        "loss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n",
        "\n",
        "LEARNING_RATE = 1e-05\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "def train():\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \"\"\"\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    total_pred, total_tokens = 0, 0\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(loader):\n",
        "\n",
        "        # Get the encoded data for this batch and push it to the GPU\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "        pred_mask = batch['pred'].to(device, dtype = torch.long)\n",
        "\n",
        "        # Run the forward pass of the model\n",
        "        logits = model(input_ids=ids, attn_mask=mask, pred_indicator=pred_mask)\n",
        "        loss = loss_function(logits.transpose(2,1), targets)\n",
        "        tr_loss += loss.item()\n",
        "        # print(\"Batch loss: \", loss.item()) # can comment out if too verbose.\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        # Compute accuracy for this batch\n",
        "        matching = torch.sum(torch.argmax(logits,dim=2) == targets)\n",
        "        predictions = torch.sum(torch.where(targets==-100,0,1))\n",
        "        total_pred += matching\n",
        "        total_tokens += predictions\n",
        "\n",
        "        if idx % 100==0:\n",
        "          #torch.cuda.empty_cache() # can help if you run into memory issues\n",
        "          curr_avg_loss = tr_loss/nb_tr_steps\n",
        "          print(f\"Current average loss: {curr_avg_loss}\")\n",
        "          # todo\n",
        "          accuracy = total_pred/total_tokens if total_tokens > 0 else 0\n",
        "          print(f\"Current accuracy: {accuracy}\")\n",
        "\n",
        "        # Run the backward pass to update parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    epoch_accuracy = total_pred/total_tokens if total_tokens > 0 else 0\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy: {epoch_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890d12b0",
      "metadata": {
        "id": "890d12b0"
      },
      "source": [
        "Now let's train the model for one epoch. This will take an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef88882",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bef88882",
        "outputId": "d9b5387e-3aa4-4325-a863-3c205a816187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current average loss: 4.1920061111450195\n",
            "Current accuracy: 0.006472492124885321\n",
            "Current average loss: 1.9889625499744226\n",
            "Current accuracy: 0.604480504989624\n",
            "Current average loss: 1.7254493820726575\n",
            "Current accuracy: 0.6329650282859802\n",
            "Current average loss: 1.6067084103723697\n",
            "Current accuracy: 0.6407582759857178\n",
            "Current average loss: 1.4809888152410264\n",
            "Current accuracy: 0.6624303460121155\n",
            "Current average loss: 1.3675114648070879\n",
            "Current accuracy: 0.6844727396965027\n",
            "Current average loss: 1.274774910011228\n",
            "Current accuracy: 0.7027723789215088\n",
            "Current average loss: 1.1948177507957616\n",
            "Current accuracy: 0.718860924243927\n",
            "Current average loss: 1.1289339042855857\n",
            "Current accuracy: 0.7322730422019958\n",
            "Current average loss: 1.0724443235619616\n",
            "Current accuracy: 0.7442576289176941\n",
            "Current average loss: 1.0243269722838027\n",
            "Current accuracy: 0.7542441487312317\n",
            "Current average loss: 0.9795466253261583\n",
            "Current accuracy: 0.7637176513671875\n",
            "Current average loss: 0.9395483292012687\n",
            "Current accuracy: 0.772463321685791\n",
            "Current average loss: 0.9048877293380015\n",
            "Current accuracy: 0.7798197865486145\n",
            "Current average loss: 0.8742466837401733\n",
            "Current accuracy: 0.7863019108772278\n",
            "Current average loss: 0.8468609511614005\n",
            "Current accuracy: 0.7922567129135132\n",
            "Current average loss: 0.8219419741522588\n",
            "Current accuracy: 0.797470211982727\n",
            "Current average loss: 0.7977946481586974\n",
            "Current accuracy: 0.8028613328933716\n",
            "Current average loss: 0.7766139021123667\n",
            "Current accuracy: 0.8072001934051514\n",
            "Current average loss: 0.7574371035901074\n",
            "Current accuracy: 0.8112819790840149\n",
            "Current average loss: 0.7383310655946436\n",
            "Current accuracy: 0.8154184818267822\n",
            "Current average loss: 0.721010901902699\n",
            "Current accuracy: 0.8191110491752625\n",
            "Current average loss: 0.7051010366693403\n",
            "Current accuracy: 0.8225377202033997\n",
            "Current average loss: 0.6895370076255455\n",
            "Current accuracy: 0.8259693384170532\n",
            "Current average loss: 0.6744793708289181\n",
            "Current accuracy: 0.8293917775154114\n",
            "Current average loss: 0.6620415506280455\n",
            "Current accuracy: 0.8320263028144836\n",
            "Current average loss: 0.6492514117079852\n",
            "Current accuracy: 0.8348844647407532\n",
            "Current average loss: 0.6387109568563932\n",
            "Current accuracy: 0.8372502326965332\n",
            "Current average loss: 0.6281674398200421\n",
            "Current accuracy: 0.8395438194274902\n",
            "Current average loss: 0.6167737937916644\n",
            "Current accuracy: 0.8421333432197571\n",
            "Current average loss: 0.6071741725555542\n",
            "Current accuracy: 0.8442750573158264\n",
            "Current average loss: 0.5977269042426253\n",
            "Current accuracy: 0.8462951183319092\n",
            "Current average loss: 0.5892671095844508\n",
            "Current accuracy: 0.8482109308242798\n",
            "Current average loss: 0.5815576099037618\n",
            "Current accuracy: 0.8499460220336914\n",
            "Current average loss: 0.574052443150053\n",
            "Current accuracy: 0.8515207171440125\n",
            "Current average loss: 0.5659007999712963\n",
            "Current accuracy: 0.8533003926277161\n",
            "Current average loss: 0.5585610962508288\n",
            "Current accuracy: 0.8548220992088318\n",
            "Current average loss: 0.5514310334049924\n",
            "Current accuracy: 0.8563141822814941\n",
            "Current average loss: 0.54487916372836\n",
            "Current accuracy: 0.8577651381492615\n",
            "Current average loss: 0.5391016682427041\n",
            "Current accuracy: 0.8589486479759216\n",
            "Current average loss: 0.533346453850581\n",
            "Current accuracy: 0.8601881265640259\n",
            "Current average loss: 0.5272939610400598\n",
            "Current accuracy: 0.8615767955780029\n",
            "Current average loss: 0.5219155355731069\n",
            "Current accuracy: 0.8628043532371521\n",
            "Current average loss: 0.5166372453832426\n",
            "Current accuracy: 0.8640163540840149\n",
            "Current average loss: 0.511403852617895\n",
            "Current accuracy: 0.8651891350746155\n",
            "Current average loss: 0.5065399129187841\n",
            "Current accuracy: 0.8661942481994629\n",
            "Current average loss: 0.5021700573256419\n",
            "Current accuracy: 0.8670797944068909\n",
            "Current average loss: 0.49755885848096776\n",
            "Current accuracy: 0.8681523203849792\n",
            "Current average loss: 0.4931264252473081\n",
            "Current accuracy: 0.8691484928131104\n",
            "Current average loss: 0.48870436633315484\n",
            "Current accuracy: 0.8701216578483582\n",
            "Current average loss: 0.48459131088442287\n",
            "Current accuracy: 0.8710650205612183\n",
            "Current average loss: 0.4804223556319094\n",
            "Current accuracy: 0.8719882965087891\n",
            "Current average loss: 0.4763111171757251\n",
            "Current accuracy: 0.8728889226913452\n",
            "Current average loss: 0.47248904159093347\n",
            "Current accuracy: 0.8737536668777466\n",
            "Current average loss: 0.4688450290199519\n",
            "Current accuracy: 0.8745762705802917\n",
            "Current average loss: 0.4655733916206244\n",
            "Current accuracy: 0.8752564787864685\n",
            "Current average loss: 0.4625246889504806\n",
            "Current accuracy: 0.8759254813194275\n",
            "Current average loss: 0.4593681649832093\n",
            "Current accuracy: 0.8766034245491028\n",
            "Current average loss: 0.4561467394001565\n",
            "Current accuracy: 0.8773093223571777\n",
            "Current average loss: 0.45275605894426957\n",
            "Current accuracy: 0.8781156539916992\n",
            "Current average loss: 0.4496563683982353\n",
            "Current accuracy: 0.8788295984268188\n",
            "Current average loss: 0.4465819621900421\n",
            "Current accuracy: 0.879548966884613\n",
            "Current average loss: 0.44373800330989843\n",
            "Current accuracy: 0.8801925182342529\n",
            "Current average loss: 0.44055320160104877\n",
            "Current accuracy: 0.8808779120445251\n",
            "Current average loss: 0.43763685715715783\n",
            "Current accuracy: 0.8815519213676453\n",
            "Current average loss: 0.4348931363337527\n",
            "Current accuracy: 0.8822077512741089\n",
            "Current average loss: 0.4323407592376901\n",
            "Current accuracy: 0.8827574849128723\n",
            "Current average loss: 0.4295404619774815\n",
            "Current accuracy: 0.8834022879600525\n",
            "Current average loss: 0.42727743558680953\n",
            "Current accuracy: 0.8839046955108643\n",
            "Current average loss: 0.424862675943693\n",
            "Current accuracy: 0.8844630122184753\n",
            "Current average loss: 0.42246764957204563\n",
            "Current accuracy: 0.8849862813949585\n",
            "Current average loss: 0.41978157657661447\n",
            "Current accuracy: 0.8856159448623657\n",
            "Current average loss: 0.4174821310618564\n",
            "Current accuracy: 0.8861792087554932\n",
            "Current average loss: 0.41538680456614446\n",
            "Current accuracy: 0.8866279125213623\n",
            "Current average loss: 0.41322616756882496\n",
            "Current accuracy: 0.887083888053894\n",
            "Current average loss: 0.4110550712394271\n",
            "Current accuracy: 0.8875802159309387\n",
            "Current average loss: 0.4090487940263637\n",
            "Current accuracy: 0.8880366086959839\n",
            "Current average loss: 0.40688649580316055\n",
            "Current accuracy: 0.8885602951049805\n",
            "Current average loss: 0.40498740873144934\n",
            "Current accuracy: 0.8889811635017395\n",
            "Current average loss: 0.40294242878616\n",
            "Current accuracy: 0.8894132375717163\n",
            "Current average loss: 0.40082281587722\n",
            "Current accuracy: 0.8899217247962952\n",
            "Current average loss: 0.39897895276724904\n",
            "Current accuracy: 0.8903400301933289\n",
            "Current average loss: 0.3971908977587962\n",
            "Current accuracy: 0.8907293677330017\n",
            "Current average loss: 0.3952511114854473\n",
            "Current accuracy: 0.891179621219635\n",
            "Current average loss: 0.39353137482797984\n",
            "Current accuracy: 0.8915631771087646\n",
            "Current average loss: 0.3916910515747348\n",
            "Current accuracy: 0.8919891119003296\n",
            "Current average loss: 0.3898494502841567\n",
            "Current accuracy: 0.892420768737793\n",
            "Current average loss: 0.3879361334195282\n",
            "Current accuracy: 0.8928835391998291\n",
            "Current average loss: 0.3861610112262475\n",
            "Current accuracy: 0.8932718634605408\n",
            "Current average loss: 0.38444807503944556\n",
            "Current accuracy: 0.8936904072761536\n",
            "Current average loss: 0.3828641133987795\n",
            "Current accuracy: 0.8940417766571045\n",
            "Current average loss: 0.3813456873921212\n",
            "Current accuracy: 0.8944084048271179\n",
            "Current average loss: 0.37976032890296024\n",
            "Current accuracy: 0.894767701625824\n",
            "Current average loss: 0.3782176168409391\n",
            "Current accuracy: 0.895126223564148\n",
            "Current average loss: 0.3767522149217527\n",
            "Current accuracy: 0.8954437375068665\n",
            "Current average loss: 0.37527241946833534\n",
            "Current accuracy: 0.8957590460777283\n",
            "Current average loss: 0.3737615284334091\n",
            "Current accuracy: 0.8960981965065002\n",
            "Current average loss: 0.3723454153862777\n",
            "Current accuracy: 0.8964260816574097\n",
            "Current average loss: 0.370851125991955\n",
            "Current accuracy: 0.8967622518539429\n",
            "Training loss epoch: 0.37071264082523026\n",
            "Training accuracy: 0.89677494764328\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8507d02",
      "metadata": {
        "id": "f8507d02"
      },
      "source": [
        "In my experiments, I found that two epochs are needed for good performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0070c530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0070c530",
        "outputId": "c94c3920-3763-4c6a-f0d2-d699b6f2dc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current average loss: 0.5636007785797119\n",
            "Current accuracy: 0.8828365802764893\n",
            "Current average loss: 0.20463491843478515\n",
            "Current accuracy: 0.9374982118606567\n",
            "Current average loss: 0.20679000101575803\n",
            "Current accuracy: 0.9375416040420532\n",
            "Current average loss: 0.2072513322299501\n",
            "Current accuracy: 0.9381908774375916\n",
            "Current average loss: 0.205442451033509\n",
            "Current accuracy: 0.9377928376197815\n",
            "Current average loss: 0.2043463569617795\n",
            "Current accuracy: 0.937982976436615\n",
            "Current average loss: 0.2057464688198142\n",
            "Current accuracy: 0.9375393986701965\n",
            "Current average loss: 0.20485628030652156\n",
            "Current accuracy: 0.9377273321151733\n",
            "Current average loss: 0.20760540222370075\n",
            "Current accuracy: 0.9369857907295227\n",
            "Current average loss: 0.2085018782244869\n",
            "Current accuracy: 0.9366990327835083\n",
            "Current average loss: 0.20744590518194123\n",
            "Current accuracy: 0.9368814826011658\n",
            "Current average loss: 0.20697860979728003\n",
            "Current accuracy: 0.937070369720459\n",
            "Current average loss: 0.2065544980333856\n",
            "Current accuracy: 0.9370080232620239\n",
            "Current average loss: 0.2067522040434428\n",
            "Current accuracy: 0.9370816349983215\n",
            "Current average loss: 0.20573640706484272\n",
            "Current accuracy: 0.9373304843902588\n",
            "Current average loss: 0.20566412769034653\n",
            "Current accuracy: 0.9375123381614685\n",
            "Current average loss: 0.20485504318975717\n",
            "Current accuracy: 0.9376567006111145\n",
            "Current average loss: 0.20493553173254686\n",
            "Current accuracy: 0.9375025033950806\n",
            "Current average loss: 0.20415241761067918\n",
            "Current accuracy: 0.9375932216644287\n",
            "Current average loss: 0.20391097169042197\n",
            "Current accuracy: 0.937534749507904\n",
            "Current average loss: 0.20347631747263303\n",
            "Current accuracy: 0.9377121925354004\n",
            "Current average loss: 0.20239015339187982\n",
            "Current accuracy: 0.9379867911338806\n",
            "Current average loss: 0.20228383132594654\n",
            "Current accuracy: 0.9380286335945129\n",
            "Current average loss: 0.20184965302495012\n",
            "Current accuracy: 0.9381374716758728\n",
            "Current average loss: 0.20165234214561475\n",
            "Current accuracy: 0.9381715059280396\n",
            "Current average loss: 0.2009803930290791\n",
            "Current accuracy: 0.9383466243743896\n",
            "Current average loss: 0.20113832073021926\n",
            "Current accuracy: 0.9381769895553589\n",
            "Current average loss: 0.2009767645577597\n",
            "Current accuracy: 0.938188910484314\n",
            "Current average loss: 0.20058310733857387\n",
            "Current accuracy: 0.9382455348968506\n",
            "Current average loss: 0.20016799529039092\n",
            "Current accuracy: 0.938343346118927\n",
            "Current average loss: 0.19991552390677061\n",
            "Current accuracy: 0.9384164810180664\n",
            "Current average loss: 0.1995941129137973\n",
            "Current accuracy: 0.9384326934814453\n",
            "Current average loss: 0.19952579687201988\n",
            "Current accuracy: 0.9384834170341492\n",
            "Current average loss: 0.19912432016671083\n",
            "Current accuracy: 0.9385722875595093\n",
            "Current average loss: 0.19906176353173022\n",
            "Current accuracy: 0.9385527968406677\n",
            "Current average loss: 0.1987779278566721\n",
            "Current accuracy: 0.9386244416236877\n",
            "Current average loss: 0.19852203533976187\n",
            "Current accuracy: 0.9386830925941467\n",
            "Current average loss: 0.1984085564855642\n",
            "Current accuracy: 0.9386774897575378\n",
            "Current average loss: 0.19795965227785783\n",
            "Current accuracy: 0.9388121962547302\n",
            "Current average loss: 0.1977293074132922\n",
            "Current accuracy: 0.9389032125473022\n",
            "Current average loss: 0.19737875810830155\n",
            "Current accuracy: 0.9390186071395874\n",
            "Current average loss: 0.19709658741831082\n",
            "Current accuracy: 0.9391224980354309\n",
            "Current average loss: 0.19691837444046628\n",
            "Current accuracy: 0.9391772150993347\n",
            "Current average loss: 0.1966773268338609\n",
            "Current accuracy: 0.9393205046653748\n",
            "Current average loss: 0.1963660521375621\n",
            "Current accuracy: 0.9394106864929199\n",
            "Current average loss: 0.1960957340914338\n",
            "Current accuracy: 0.9394879937171936\n",
            "Current average loss: 0.19541658561897782\n",
            "Current accuracy: 0.9396651983261108\n",
            "Current average loss: 0.19534151161704083\n",
            "Current accuracy: 0.9397161602973938\n",
            "Current average loss: 0.19513460850838546\n",
            "Current accuracy: 0.9397738575935364\n",
            "Current average loss: 0.1949563043880307\n",
            "Current accuracy: 0.939824104309082\n",
            "Current average loss: 0.19494445565354226\n",
            "Current accuracy: 0.9398640394210815\n",
            "Current average loss: 0.19471903043362562\n",
            "Current accuracy: 0.9399462342262268\n",
            "Current average loss: 0.19466370377683154\n",
            "Current accuracy: 0.9399561882019043\n",
            "Current average loss: 0.19462761603375917\n",
            "Current accuracy: 0.9399915933609009\n",
            "Current average loss: 0.19473690040639938\n",
            "Current accuracy: 0.9399858117103577\n",
            "Current average loss: 0.1946440018866522\n",
            "Current accuracy: 0.9400094151496887\n",
            "Current average loss: 0.19448455997374076\n",
            "Current accuracy: 0.940014660358429\n",
            "Current average loss: 0.19460851241440924\n",
            "Current accuracy: 0.9399727582931519\n",
            "Current average loss: 0.19455211374306963\n",
            "Current accuracy: 0.9399787187576294\n",
            "Current average loss: 0.1943902955332907\n",
            "Current accuracy: 0.9400115609169006\n",
            "Current average loss: 0.19422859720633717\n",
            "Current accuracy: 0.9400665163993835\n",
            "Current average loss: 0.19411520238363084\n",
            "Current accuracy: 0.9401261210441589\n",
            "Current average loss: 0.1941331377689386\n",
            "Current accuracy: 0.9401017427444458\n",
            "Current average loss: 0.1940496279119764\n",
            "Current accuracy: 0.9401167035102844\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-f202ecf11beb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Run the forward pass of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-337d06ac3ab2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attn_mask, pred_indicator)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Note the use of the \"token type ids\" which represents the segment encoding explained in the introduction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# In our segment encoding, 1 indicates the predicate, and 0 indicates everything else.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mbert_output\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_indicator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0menc_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the result of encoding the input with BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         )\n\u001b[0;32m-> 2900\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2901\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4005af14",
      "metadata": {
        "id": "4005af14"
      },
      "source": [
        "Training loss = 0.19\n",
        "Training accuracy = 0.94\n",
        "\n",
        "Once the model is trained, save the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542403f2",
      "metadata": {
        "id": "542403f2"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"srl_model_fulltrain_2epoch_finetune_1e-05.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dc1647",
      "metadata": {
        "id": "01dc1647"
      },
      "source": [
        "## 4. Decoding (1 hour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d7b2cc",
      "metadata": {
        "id": "44d7b2cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a99b6c-e316-4944-84fb-a74686edca79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-c7f832141aca>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n"
          ]
        }
      ],
      "source": [
        "model = SrlModel().to('cuda')\n",
        "model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n",
        "model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e07781",
      "metadata": {
        "id": "44e07781"
      },
      "source": [
        "The purpose of training a model is to decode an unseen example.\n",
        "\n",
        "How to decode:\n",
        "1. Extracts the argmax to obtain the label predictions for each token\n",
        "2. Translate the result into a list of string labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9026d718",
      "metadata": {
        "id": "9026d718"
      },
      "outputs": [],
      "source": [
        "def decode_output(logits):\n",
        "    \"\"\"\n",
        "    Given the model output, return a list of string labels for each token.\n",
        "    \"\"\"\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "    pred_indices = predictions[0].cpu().numpy()\n",
        "    pred_labels = [id_to_role[idx] for idx in pred_indices if idx in id_to_role]\n",
        "\n",
        "    return pred_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f21f29",
      "metadata": {
        "id": "76f21f29"
      },
      "outputs": [],
      "source": [
        "def label_sentence(tokens, pred_idx):\n",
        "\n",
        "    # call this function to prepare token_ids, attention mask, predicate mask, then call the model.\n",
        "    # Decode the output to produce a list of labels.\n",
        "    # Create predicate indicator mask (1 for predicate position, 0 elsewhere)\n",
        "    pred_mask = [1 if i == pred_idx else 0 for i in range(len(tokens))]\n",
        "\n",
        "    # Tokenize the input tokens\n",
        "    tokenized_tokens = []\n",
        "    token_pred_mask = []\n",
        "\n",
        "    # Handle tokenization and maintain predicate mask alignment\n",
        "    for i, token in enumerate(tokens):\n",
        "        subtokens = tokenizer.tokenize(token)\n",
        "        tokenized_tokens.extend(subtokens)\n",
        "        # Extend predicate mask for subtokens\n",
        "        if pred_mask[i] == 1:\n",
        "            token_pred_mask.extend([1] * len(subtokens))\n",
        "        else:\n",
        "            token_pred_mask.extend([0] * len(subtokens))\n",
        "\n",
        "    # Truncate if too long\n",
        "    max_len = 128 - 2  # Account for [CLS] and [SEP]\n",
        "    if len(tokenized_tokens) > max_len:\n",
        "        tokenized_tokens = tokenized_tokens[:max_len]\n",
        "        token_pred_mask = token_pred_mask[:max_len]\n",
        "\n",
        "    # Add special tokens\n",
        "    tokenized_tokens = ['[CLS]'] + tokenized_tokens + ['[SEP]']\n",
        "    token_pred_mask = [0] + token_pred_mask + [0]\n",
        "\n",
        "    # Pad to max length\n",
        "    padding_length = 128 - len(tokenized_tokens)\n",
        "    tokenized_tokens.extend(['[PAD]'] * padding_length)\n",
        "    token_pred_mask.extend([0] * padding_length)\n",
        "    attention_mask = [1] * (len(tokenized_tokens) - padding_length) + [0] * padding_length\n",
        "\n",
        "    # Convert tokens to ids\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokenized_tokens)\n",
        "\n",
        "    # Create tensors and move to GPU\n",
        "    ids = torch.tensor([token_ids], dtype=torch.long).to('cuda')\n",
        "    mask = torch.tensor([attention_mask], dtype=torch.long).to('cuda')\n",
        "    pred = torch.tensor([token_pred_mask], dtype=torch.long).to('cuda')\n",
        "\n",
        "    # Get model predictions\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=ids, attn_mask=mask, pred_indicator=pred)\n",
        "\n",
        "    # Decode predictions\n",
        "    predictions = decode_output(outputs)\n",
        "\n",
        "    # Since model output includes predictions for [CLS], [SEP], and padding,\n",
        "    # we need to align the predictions back to the original tokens\n",
        "    aligned_predictions = []\n",
        "    token_idx = 0\n",
        "    pred_idx = 1  # Start after [CLS]\n",
        "\n",
        "    while token_idx < len(tokens) and pred_idx < len(predictions) - 1:  # -1 to avoid [SEP]\n",
        "        aligned_predictions.append(predictions[pred_idx])\n",
        "        subtokens = tokenizer.tokenize(tokens[token_idx])\n",
        "        pred_idx += len(subtokens)\n",
        "        token_idx += 1\n",
        "\n",
        "    # If we didn't get predictions for all tokens (possible if sentence was truncated)\n",
        "    while len(aligned_predictions) < len(tokens):\n",
        "        aligned_predictions.append('O')\n",
        "\n",
        "    return aligned_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98431b63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98431b63",
        "outputId": "654458dc-7476-480e-bf63-8be074659519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(A, O),\n",
            "(U., O),\n",
            "(N., O),\n",
            "(team, O),\n",
            "(spent, O),\n",
            "(an, O),\n",
            "(hour, O),\n",
            "(inside, O),\n",
            "(the, B-ARGM-LOC),\n",
            "(hospital, I-ARGM-LOC),\n",
            "(,, O),\n",
            "(where, B-ARGM-LOC),\n",
            "(it, B-ARG0),\n",
            "(found, B-V),\n",
            "(evident, B-ARG1),\n",
            "(signs, I-ARG1),\n",
            "(of, I-ARG1),\n",
            "(shelling, I-ARG1),\n",
            "(and, I-ARG1),\n",
            "(gunfire, I-ARG1),\n",
            "(., O),\n"
          ]
        }
      ],
      "source": [
        "tokens = \"A U. N. team spent an hour inside the hospital , where it found evident signs of shelling and gunfire .\".split()\n",
        "# Now you should be able to run\n",
        "label_test = label_sentence(tokens, 13) # Predicate is \"found\"\n",
        "zip(tokens, label_test)\n",
        "for token, label in zip(tokens, label_test):\n",
        "    print(f\"({token}, {label}),\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6c710b",
      "metadata": {
        "id": "1d6c710b"
      },
      "source": [
        "The expected output is (pulled from results):\n",
        "```   \n",
        " ('A', 'O'),\n",
        " ('U.', 'O'),\n",
        " ('N.', 'O'),\n",
        " ('team', 'O'),\n",
        " ('spent', 'O'),\n",
        " ('an', 'O'),\n",
        " ('hour', 'O'),\n",
        " ('inside', 'O'),\n",
        " ('the', 'B-ARGM-LOC'),\n",
        " ('hospital', 'I-ARGM-LOC'),\n",
        " (',', 'O'),\n",
        " ('where', 'B-ARGM-LOC'),\n",
        " ('it', 'B-ARG0'),\n",
        " ('found', 'B-V'),\n",
        " ('evident', 'B-ARG1'),\n",
        " ('signs', 'I-ARG1'),\n",
        " ('of', 'I-ARG1'),\n",
        " ('shelling', 'I-ARG1'),\n",
        " ('and', 'I-ARG1'),\n",
        " ('gunfire', 'I-ARG1'),\n",
        " ('.', 'O'),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c0237e",
      "metadata": {
        "id": "b6c0237e"
      },
      "source": [
        "## 5. Token-based Accuracy: (30 mins)\n",
        "We want to evaluate the model on the dev or test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7aa897",
      "metadata": {
        "id": "cc7aa897"
      },
      "outputs": [],
      "source": [
        "dev_data = SrlData(\"propbank_dev.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd62569e",
      "metadata": {
        "id": "dd62569e"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dev_data, batch_size = 1, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8ababd",
      "metadata": {
        "id": "cc8ababd"
      },
      "source": [
        "Validation is done on every finetuning task, this is no different\n",
        "\n",
        "The evaluate_token_accuracy function:\n",
        "1. iterate through the items in the data loader\n",
        "2. run the model on each sentence\n",
        "3. extract the predictions\n",
        "\n",
        "For each sentence, we count the correct predictions and total predictions.\n",
        "\n",
        "accuracy = correct_predictions / total_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa091465",
      "metadata": {
        "id": "fa091465"
      },
      "outputs": [],
      "source": [
        "def evaluate_token_accuracy(model, loader):\n",
        "\n",
        "    model.eval() # put model in evaluation mode\n",
        "\n",
        "    # for the accuracy\n",
        "    total_correct = 0 # number of correct token label predictions.\n",
        "    total_predictions = 0 # number of total predictions = number of tokens in the data.\n",
        "\n",
        "    # iterate over the data here.\n",
        "    # todo\n",
        "    with torch.no_grad():  # disable gradient computation for evaluation\n",
        "      for idx, batch in enumerate(loader):\n",
        "        # Get batch data and move to GPU\n",
        "        ids = batch['ids'].to(device, dtype=torch.long)\n",
        "        mask = batch['mask'].to(device, dtype=torch.long)\n",
        "        targets = batch['targets'].to(device, dtype=torch.long)\n",
        "        pred_mask = batch['pred'].to(device, dtype=torch.long)\n",
        "\n",
        "        # Get model predictions\n",
        "        logits = model(input_ids=ids, attn_mask=mask, pred_indicator=pred_mask)\n",
        "        predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "        # Create a mask for non-padded tokens (where target != -100)\n",
        "        non_pad_mask = targets != -100\n",
        "\n",
        "        # Count correct predictions (excluding padded tokens)\n",
        "        correct = ((predictions == targets) & non_pad_mask).sum().item()\n",
        "        total = non_pad_mask.sum().item()\n",
        "\n",
        "        total_correct += correct\n",
        "        total_predictions += total\n",
        "\n",
        "    acc = total_correct / total_predictions\n",
        "    print(f\"Accuracy: {acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_loader = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
        "accuracy = evaluate_token_accuracy(model, dev_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvVaEX5FVzJ",
        "outputId": "d7c8f6ad-c528-4568-c7cf-28dba12fcd33"
      },
      "id": "TTvVaEX5FVzJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9354921171673937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb27239",
      "metadata": {
        "id": "9fb27239"
      },
      "source": [
        "## 6. Sentence-based accuracy (30 mins)\n",
        "\n",
        "SRL systems are typically evaluated on micro-averaged precision, recall, and F1-score for predicting labeled spans.\n",
        "\n",
        "For each sentence/predicate input, we run the model, decode the output, and extract a set of labeled spans (from the output and the target labels). These spans are (i,j,label) tuples.  \n",
        "\n",
        "We then compute the true_positives, false_positives, and false_negatives based on these spans.\n",
        "\n",
        "In the end, we can compute\n",
        "\n",
        "* Precision:  true_positive / (true_positives + false_positives)  , that is the number of correct spans out of all predicted spans.\n",
        "\n",
        "* Recall: true_positives / (true_positives + false_negatives) , that is the number of correct spans out of all target spans.\n",
        "\n",
        "* F1-score:   (2 * precision * recall) / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379cfe6a",
      "metadata": {
        "id": "379cfe6a"
      },
      "outputs": [],
      "source": [
        "def extract_spans(labels):\n",
        "    spans = {} # map (start,end) ids to label\n",
        "    current_span_start = 0\n",
        "    current_span_type = \"\"\n",
        "    inside = False\n",
        "    for i, label in enumerate(labels):\n",
        "        if label.startswith(\"B\"):\n",
        "            if inside:\n",
        "                if current_span_type != \"V\":\n",
        "                    spans[(current_span_start,i)] = current_span_type\n",
        "            current_span_start = i\n",
        "            current_span_type = label[2:]\n",
        "            inside = True\n",
        "        elif inside and label.startswith(\"O\"):\n",
        "            if current_span_type != \"V\":\n",
        "                spans[(current_span_start,i)] = current_span_type\n",
        "            inside = False\n",
        "        elif inside and label.startswith(\"I\") and label[2:] != current_span_type:\n",
        "            if current_span_type != \"V\":\n",
        "                spans[(current_span_start,i)] = current_span_type\n",
        "            inside = False\n",
        "    return spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5486ca",
      "metadata": {
        "id": "1a5486ca"
      },
      "outputs": [],
      "source": [
        "def evaluate_spans(model, loader):\n",
        "    model.eval()  # put model in evaluation mode\n",
        "\n",
        "    total_tp = 0  # true positives\n",
        "    total_fp = 0  # false positives\n",
        "    total_fn = 0  # false negatives\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(loader):\n",
        "            # Get batch data and move to GPU\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "            pred_mask = batch['pred'].to(device, dtype=torch.long)\n",
        "\n",
        "            # Get model predictions\n",
        "            logits = model(input_ids=ids, attn_mask=mask, pred_indicator=pred_mask)\n",
        "            predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "            # Convert predictions and targets to lists of labels\n",
        "            pred_labels = [id_to_role[p.item()] for p in predictions[0] if p.item() in id_to_role]\n",
        "            target_labels = [id_to_role[t.item()] for t in targets[0] if t.item() != -100]\n",
        "\n",
        "            # Extract spans from predictions and targets\n",
        "            pred_spans = extract_spans(pred_labels)\n",
        "            target_spans = extract_spans(target_labels)\n",
        "\n",
        "            # Calculate metrics for this batch\n",
        "            # True positives: spans that appear in both pred and target with same label\n",
        "            tp = sum(1 for span, label in pred_spans.items()\n",
        "                    if span in target_spans and target_spans[span] == label)\n",
        "\n",
        "            # False positives: spans in pred that aren't in target or have wrong label\n",
        "            fp = sum(1 for span, label in pred_spans.items()\n",
        "                    if span not in target_spans or target_spans[span] != label)\n",
        "\n",
        "            # False negatives: spans in target that aren't in pred or have wrong label\n",
        "            fn = sum(1 for span, label in target_spans.items()\n",
        "                    if span not in pred_spans or pred_spans[span] != label)\n",
        "\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "    # Calculate precision, recall, and F1\n",
        "    if total_tp + total_fp == 0:\n",
        "        total_p = 0\n",
        "    else:\n",
        "        total_p = total_tp / (total_tp + total_fp)\n",
        "\n",
        "    if total_tp + total_fn == 0:\n",
        "        total_r = 0\n",
        "    else:\n",
        "        total_r = total_tp / (total_tp + total_fn)\n",
        "\n",
        "    if total_p + total_r == 0:\n",
        "        total_f = 0\n",
        "    else:\n",
        "        total_f = (2 * total_p * total_r) / (total_p + total_r)\n",
        "\n",
        "    print(f\"Overall P: {total_p:.4f}  Overall R: {total_r:.4f}  Overall F1: {total_f:.4f}\")\n",
        "    return total_p, total_r, total_f"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_spans(model, loader)"
      ],
      "metadata": {
        "id": "0mICAfbmHYLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be674675-29a8-42e0-d4c8-cfcfde07b043"
      },
      "id": "0mICAfbmHYLq",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall P: 0.2076  Overall R: 0.8259  Overall F1: 0.3318\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.20757975935940973, 0.825866497648197, 0.33176987711234107)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e35d04",
      "metadata": {
        "id": "c0e35d04"
      },
      "source": [
        "The F score = 0.82  (which slightly below the state-of-the art in 2018)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93bd7099ad4f4fd5a18b93d27247c0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44a9ac7c64a84fca8f5283ad0e856406",
              "IPY_MODEL_3eb43ac363c64a8f8591b095f6031124",
              "IPY_MODEL_b929cca6f52d4fd885d51487dc879b07"
            ],
            "layout": "IPY_MODEL_0642828a79604cd49167cc673cab6109"
          }
        },
        "44a9ac7c64a84fca8f5283ad0e856406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14746cf2be3f4b7fbbf77b2659ec35bb",
            "placeholder": "​",
            "style": "IPY_MODEL_e13c9ccf399f4624bdcbf833760e6d43",
            "value": "model.safetensors: 100%"
          }
        },
        "3eb43ac363c64a8f8591b095f6031124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c50ded0e55b84ea986c8fea8aed388f3",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bdb93578f27452d9392ecd29c44a018",
            "value": 440449768
          }
        },
        "b929cca6f52d4fd885d51487dc879b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a54fd4d01549f58161edd13281756e",
            "placeholder": "​",
            "style": "IPY_MODEL_849127cb883140758caeb4db0fbeff27",
            "value": " 440M/440M [00:01&lt;00:00, 238MB/s]"
          }
        },
        "0642828a79604cd49167cc673cab6109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14746cf2be3f4b7fbbf77b2659ec35bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13c9ccf399f4624bdcbf833760e6d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c50ded0e55b84ea986c8fea8aed388f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bdb93578f27452d9392ecd29c44a018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74a54fd4d01549f58161edd13281756e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849127cb883140758caeb4db0fbeff27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
